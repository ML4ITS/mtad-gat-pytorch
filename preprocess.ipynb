{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "35554425-c930-4a9f-9c29-7f6d26857743",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from ast import literal_eval\n",
    "from csv import reader\n",
    "from os import listdir, makedirs, path\n",
    "from pickle import dump\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#from args import get_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d4abf4aa-3e49-48b7-813e-3c0707c654a0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class args_class:\n",
    "    def __init__(self,scaler = 'MinMaxScaler', cut = 1, resample_rate = 1, train_test_split = 0.7, spectral_residual = False, no_anomaly_train = True):\n",
    "        self.cut = cut\n",
    "        self.resample_rate = resample_rate\n",
    "        self.train_test_split = train_test_split\n",
    "        self.scaler = scaler\n",
    "        self.spectral_residual = spectral_residual\n",
    "        self.no_anomaly_train = no_anomaly_train\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce3a7909-5a0a-44ae-9824-22482c5a7577",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "abb1b6f2-9209-40b3-ac4e-0581b7fdd2e7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_and_save(category, filename, dataset, dataset_folder, output_folder):\n",
    "    temp = np.genfromtxt(\n",
    "        path.join(dataset_folder, category, filename),\n",
    "        dtype=np.float32,\n",
    "        delimiter=\",\",\n",
    "    )\n",
    "    print(dataset, category, filename, temp.shape)\n",
    "    with open(path.join(output_folder, dataset + \"_\" + category + \".pkl\"), \"wb\") as file:\n",
    "        dump(temp, file)\n",
    "\n",
    "\n",
    "def load_data(dataset):\n",
    "    \"\"\" Method from OmniAnomaly (https://github.com/NetManAIOps/OmniAnomaly) \"\"\"\n",
    "\n",
    "    if dataset == \"SMD\":\n",
    "        dataset_folder = \"datasets/ServerMachineDataset\"\n",
    "        output_folder = \"datasets/ServerMachineDataset/processed\"\n",
    "        makedirs(output_folder, exist_ok=True)\n",
    "        file_list = listdir(path.join(dataset_folder, \"train\"))\n",
    "        for filename in file_list:\n",
    "            if filename.endswith(\".txt\"):\n",
    "                load_and_save(\n",
    "                    \"train\",\n",
    "                    filename,\n",
    "                    filename.strip(\".txt\"),\n",
    "                    dataset_folder,\n",
    "                    output_folder,\n",
    "                )\n",
    "                load_and_save(\n",
    "                    \"test_label\",\n",
    "                    filename,\n",
    "                    filename.strip(\".txt\"),\n",
    "                    dataset_folder,\n",
    "                    output_folder,\n",
    "                )\n",
    "                load_and_save(\n",
    "                    \"test\",\n",
    "                    filename,\n",
    "                    filename.strip(\".txt\"),\n",
    "                    dataset_folder,\n",
    "                    output_folder,\n",
    "                )\n",
    "\n",
    "    elif dataset == \"SMAP\" or dataset == \"MSL\":\n",
    "        dataset_folder = \"datasets/data\"\n",
    "        output_folder = \"datasets/data/processed\"\n",
    "        makedirs(output_folder, exist_ok=True)\n",
    "        with open(path.join(dataset_folder, \"labeled_anomalies.csv\"), \"r\") as file:\n",
    "            csv_reader = reader(file, delimiter=\",\")\n",
    "            res = [row for row in csv_reader][1:]\n",
    "        res = sorted(res, key=lambda k: k[0])\n",
    "        data_info = [row for row in res if row[1] == dataset and row[0] != \"P-2\"]\n",
    "        labels = []\n",
    "        for row in data_info:\n",
    "            anomalies = literal_eval(row[2])\n",
    "            length = int(row[-1])\n",
    "            label = np.zeros([length], dtype=np.bool_)\n",
    "            for anomaly in anomalies:\n",
    "                label[anomaly[0] : anomaly[1] + 1] = True\n",
    "            labels.extend(label)\n",
    "\n",
    "        labels = np.asarray(labels)\n",
    "        print(dataset, \"test_label\", labels.shape)\n",
    "\n",
    "        with open(path.join(output_folder, dataset + \"_\" + \"test_label\" + \".pkl\"), \"wb\") as file:\n",
    "            dump(labels, file)\n",
    "\n",
    "        def concatenate_and_save(category):\n",
    "            data = []\n",
    "            for row in data_info:\n",
    "                filename = row[0]\n",
    "                temp = np.load(path.join(dataset_folder, category, filename + \".npy\"))\n",
    "                data.extend(temp)\n",
    "            data = np.asarray(data)\n",
    "            print(dataset, category, data.shape)\n",
    "            with open(path.join(output_folder, dataset + \"_\" + category + \".pkl\"), \"wb\") as file:\n",
    "                dump(data, file)\n",
    "\n",
    "        for c in [\"train\", \"test\"]:\n",
    "            concatenate_and_save(c)\n",
    "    elif dataset ==\"SWAT\":\n",
    "        swat = pd.read_csv(path.join('datasets/data', 'SWaT_Dataset_Attack_v0.csv'))\n",
    "        swat = swat.drop(' Timestamp', axis=1)\n",
    "        \n",
    "        if args.cut < 1:\n",
    "            print('Cutting the dataset at ' + str(args.cut) + ' length \\n')\n",
    "            swat = swat.iloc[:int(len(swat)*args.cut)]\n",
    "        sample_rate = args.resample_rate\n",
    "        if sample_rate<=0 or sample_rate>1:\n",
    "            print('Incorrect resample rate, defaulting to 1\\n')\n",
    "            sample_rate = 1\n",
    "        else:\n",
    "            print('resampling to one observation every '+ str(int(1/sample_rate)))\n",
    "        \n",
    "\n",
    "        swat = swat.iloc[::int(1/sample_rate)]#resampling\n",
    "        labels = (swat['Normal/Attack'].values=='Attack')\n",
    "        values = swat.drop('Normal/Attack', axis=1).values\n",
    "        \n",
    "        train_test_split=args.train_test_split\n",
    "\n",
    "        \n",
    "        if args.scaler == 'quantile':\n",
    "            from sklearn.preprocessing  import QuantileTransformer\n",
    "            scaler = QuantileTransformer(output_distribution='normal')\n",
    "        else:\n",
    "            from sklearn.preprocessing  import MinMaxScaler\n",
    "            scaler = MinMaxScaler()\n",
    "        \n",
    "        values = scaler.fit_transform(values) \n",
    "        #spectral residual data cleaning\n",
    "        if args.spectral_residual:\n",
    "            for i in range(values.shape[1]):\n",
    "                values[:,i] = spectral_residual_replace(values[:,i])\n",
    "\n",
    "        train_values = values[:int(train_test_split*len(labels)),:]\n",
    "        train_labels = labels[:int(train_test_split*len(labels))]\n",
    "\n",
    "        if args.no_anomaly_train:\n",
    "            print('removing anomalies from training data')\n",
    "            train_values = train_values[train_labels==False]\n",
    "\n",
    "        test_values = values[int(train_test_split*len(labels)):,:]\n",
    "        test_labels = labels[int(train_test_split*len(labels)):]\n",
    "\n",
    "        #dump train values into file\n",
    "        makedirs('datasets/data/processed', exist_ok=True)\n",
    "        path_pkl = path.join('datasets/data/processed', 'SWAT_train.pkl')\n",
    "        with open(path_pkl, 'wb') as file:\n",
    "            dump(train_values, file)\n",
    "\n",
    "\n",
    "\n",
    "        #dump test values into file\n",
    "        path_pkl = path.join('datasets/data/processed', 'SWAT_test.pkl')\n",
    "        with open(path_pkl, 'wb') as file:\n",
    "            dump(test_values, file)\n",
    "\n",
    "\n",
    "        #dump test labels into file\n",
    "        path_pkl = path.join('datasets/data/processed', 'SWAT_test_label.pkl')\n",
    "        with open(path_pkl, 'wb') as file:\n",
    "           dump(test_labels, file)\n",
    "    elif dataset ==\"SKAB\":\n",
    "        ## import ##\n",
    "        skab_no_attack = pd.read_csv(path.join('datasets/data/SKAB', 'anomaly-free.csv'), delimiter=';')\n",
    "        skab_no_attack = skab_no_attack.drop('datetime', axis=1)\n",
    "\n",
    "        skab_attack = pd.read_csv(path.join('datasets/data/SKAB/attacks', '1.csv'),  delimiter=';')\n",
    "        skab_attack = skab_attack.drop('datetime', axis=1)\n",
    "        skab_attack = skab_attack.drop('changepoint', axis=1)\n",
    "        \n",
    "        ## cutting ##\n",
    "        if args.cut<1:\n",
    "            print('Cutting the dataset at ' + str(args.cut) + ' length \\n')\n",
    "            skab_no_attack = skab_no_attack.iloc[:int(len(skab_no_attack)*args.cut)]\n",
    "        \n",
    "        ## resampling ##\n",
    "        sample_rate = args.resample_rate\n",
    "        if sample_rate<=0 or sample_rate>1:\n",
    "            print('Incorrect resample rate, defaulting to 1\\n')\n",
    "            sample_rate = 1\n",
    "        else:\n",
    "            print('resampling to one observation every '+ str(int(1/sample_rate)))\n",
    "        skab_no_attack = skab_no_attack.iloc[::int(1/sample_rate)]#resampling\n",
    "        skab_attack = skab_attack.iloc[::int(1/sample_rate)]#resampling\n",
    "        \n",
    "        train_values = skab_no_attack.values\n",
    "        test_values = skab_attack.drop('anomaly', axis=1).values\n",
    "        test_labels = (skab_attack['anomaly'].values==1)\n",
    "\n",
    "        ## scaling ##\n",
    "        if args.scaler == 'quantile':\n",
    "            from sklearn.preprocessing  import QuantileTransformer\n",
    "            scaler = QuantileTransformer(output_distribution='normal')\n",
    "        else:\n",
    "            from sklearn.preprocessing  import MinMaxScaler\n",
    "            scaler = MinMaxScaler()\n",
    "        \n",
    "        train_values = scaler.fit_transform(train_values)\n",
    "        test_values = scaler.transform(test_values)\n",
    "\n",
    "        #dump train values into file\n",
    "        makedirs('datasets/data/processed', exist_ok=True)\n",
    "        path_pkl = path.join('datasets/data/processed', 'SKAB_train.pkl')\n",
    "        with open(path_pkl, 'wb') as file:\n",
    "            dump(train_values, file)\n",
    "\n",
    "        #dump test values into file\n",
    "        path_pkl = path.join('datasets/data/processed', 'SKAB_test.pkl')\n",
    "        with open(path_pkl, 'wb') as file:\n",
    "            dump(test_values, file)\n",
    "\n",
    "        #dump test labels into file\n",
    "        path_pkl = path.join('datasets/data/processed', 'SKAB_test_label.pkl')\n",
    "        with open(path_pkl, 'wb') as file:\n",
    "           dump(test_labels, file)\n",
    "    elif dataset=='WADI':\n",
    "        \n",
    "        wadi = pd.read_csv(path.join('datasets/data', 'WADI_attackdataLABLE.csv'), delimiter=',', skiprows=1 )\n",
    "        wadi = wadi.drop('Row ', axis=1)\n",
    "        wadi = wadi.drop('Date ', axis=1)\n",
    "        wadi = wadi.drop('Time', axis=1)\n",
    "        wadi = wadi.drop('2_LS_001_AL', axis=1) #nan column\n",
    "        wadi = wadi.drop('2_LS_002_AL', axis=1) #nan column\n",
    "        wadi = wadi.drop('2_P_001_STATUS', axis=1) #nan column\n",
    "        wadi = wadi.drop('2_P_002_STATUS', axis=1) #nan column\n",
    "        wadi = wadi.dropna(axis=0)\n",
    "\n",
    "        if args.cut < 1:\n",
    "            print('Cutting the dataset at ' + str(args.cut) + ' length \\n')\n",
    "            wadi = wadi.iloc[:int(len(wadi)*args.cut)]\n",
    "        sample_rate = args.resample_rate\n",
    "        if sample_rate<=0 or sample_rate>1:\n",
    "            print('Incorrect resample rate, defaulting to 1\\n')\n",
    "            sample_rate = 1\n",
    "        else:\n",
    "            print('resampling to one observation every '+ str(int(1/sample_rate)))\n",
    "\n",
    "        wadi = wadi.iloc[::int(1/sample_rate)]#resampling\n",
    "        labels = (wadi['Attack LABLE (1:No Attack, -1:Attack)'].values==-1)\n",
    "        values = wadi.drop('Attack LABLE (1:No Attack, -1:Attack)', axis=1).values\n",
    "        \n",
    "        train_test_split=args.train_test_split\n",
    "\n",
    "        if args.scaler == 'quantile':\n",
    "            from sklearn.preprocessing  import QuantileTransformer\n",
    "            scaler = QuantileTransformer(output_distribution='uniform')\n",
    "        if args.scaler =='standard':\n",
    "            from sklearn.preprocessing  import StandardScaler\n",
    "            scaler = StandardScaler()\n",
    "        else:\n",
    "            from sklearn.preprocessing  import MinMaxScaler\n",
    "            scaler = MinMaxScaler()\n",
    "        \n",
    "        values = scaler.fit_transform(values) \n",
    "        #spectral residual data cleaning\n",
    "        if args.spectral_residual:\n",
    "            for i in range(values.shape[1]):\n",
    "                values[:,i] = spectral_residual_replace(values[:,i])\n",
    "\n",
    "        train_values = values[:int(train_test_split*len(labels)),:]\n",
    "        train_labels = labels[:int(train_test_split*len(labels))]\n",
    "\n",
    "        if args.no_anomaly_train:\n",
    "            print('removing anomalies from training data')\n",
    "            train_values = train_values[train_labels==False]\n",
    "\n",
    "        test_values = values[int(train_test_split*len(labels)):,:]\n",
    "        test_labels = labels[int(train_test_split*len(labels)):]\n",
    "\n",
    "        #dump train values into file\n",
    "        makedirs('datasets/data/processed', exist_ok=True)\n",
    "        path_pkl = path.join('datasets/data/processed', 'WADI_train.pkl')\n",
    "        with open(path_pkl, 'wb') as file:\n",
    "            dump(train_values, file)\n",
    "\n",
    "\n",
    "\n",
    "        #dump test values into file\n",
    "        path_pkl = path.join('datasets/data/processed', 'WADI_test.pkl')\n",
    "        with open(path_pkl, 'wb') as file:\n",
    "            dump(test_values, file)\n",
    "\n",
    "\n",
    "        #dump test labels into file\n",
    "        path_pkl = path.join('datasets/data/processed', 'WADI_test_label.pkl')\n",
    "        with open(path_pkl, 'wb') as file:\n",
    "           dump(test_labels, file)\n",
    "    elif dataset ==\"ACT\":\n",
    "        X_1 = pd.read_csv(path.join('datasets/data/ACT/Train', 'X_train.txt'), delimiter=' ', header=None)\n",
    "        X_2 = pd.read_csv(path.join('datasets/data/ACT/Test', 'X_test.txt'), delimiter=' ', header=None)\n",
    "        values = pd.concat([X_1, X_2], axis=0, ignore_index=True)\n",
    "\n",
    "        y_1 = pd.read_csv(path.join('datasets/data/ACT/Train', 'y_train.txt'), delimiter=' ', header=None)\n",
    "        y_2 = pd.read_csv(path.join('datasets/data/ACT/Test', 'y_test.txt'), delimiter=' ', header=None)\n",
    "        y = pd.concat([y_1, y_2], axis=0, ignore_index=True)\n",
    "        labels = np.array([x in range(7,13) for x in y.values])\n",
    "\n",
    "        if args.cut < 1:\n",
    "            print('Cutting the dataset at ' + str(args.cut) + ' length \\n')\n",
    "            values = values.iloc[:int(len(values)*args.cut)]\n",
    "            labels = labels[:int(len(labels)*args.cut)]\n",
    "        sample_rate = args.resample_rate\n",
    "        if sample_rate<=0 or sample_rate>1:\n",
    "            print('Incorrect resample rate, defaulting to 1\\n')\n",
    "            sample_rate = 1\n",
    "        else:\n",
    "            print('resampling to one observation every '+ str(int(1/sample_rate)))\n",
    "\n",
    "        values = values.iloc[::int(1/sample_rate)].values#resampling\n",
    "        labels = labels[::int(1/sample_rate)]#resampling\n",
    "\n",
    "        train_test_split=args.train_test_split\n",
    "\n",
    "        if args.scaler == 'quantile':\n",
    "            from sklearn.preprocessing  import QuantileTransformer\n",
    "            scaler = QuantileTransformer(output_distribution='uniform')\n",
    "        if args.scaler =='standard':\n",
    "            from sklearn.preprocessing  import StandardScaler\n",
    "            scaler = StandardScaler()\n",
    "        else:\n",
    "            from sklearn.preprocessing  import MinMaxScaler\n",
    "            scaler = MinMaxScaler()\n",
    "        \n",
    "        values = scaler.fit_transform(values) \n",
    "        #spectral residual data cleaning\n",
    "        if args.spectral_residual:\n",
    "            for i in range(values.shape[1]):\n",
    "                values[:,i] = spectral_residual_replace(values[:,i])\n",
    "\n",
    "        train_values = values[:int(train_test_split*len(labels)),:]\n",
    "        train_labels = labels[:int(train_test_split*len(labels))]\n",
    "\n",
    "        if args.no_anomaly_train:\n",
    "            print('removing anomalies from training data')\n",
    "            train_values = train_values[train_labels==False]\n",
    "\n",
    "        test_values = values[int(train_test_split*len(labels)):,:]\n",
    "        test_labels = labels[int(train_test_split*len(labels)):]\n",
    "\n",
    "        #dump train values into file\n",
    "        makedirs('datasets/data/processed', exist_ok=True)\n",
    "        path_pkl = path.join('datasets/data/processed', 'ACT_train.pkl')\n",
    "        with open(path_pkl, 'wb') as file:\n",
    "            dump(train_values, file)\n",
    "\n",
    "\n",
    "\n",
    "        #dump test values into file\n",
    "        path_pkl = path.join('datasets/data/processed', 'ACT_test.pkl')\n",
    "        with open(path_pkl, 'wb') as file:\n",
    "            dump(test_values, file)\n",
    "\n",
    "\n",
    "        #dump test labels into file\n",
    "        path_pkl = path.join('datasets/data/processed', 'ACT_test_label.pkl')\n",
    "        with open(path_pkl, 'wb') as file:\n",
    "           dump(test_labels, file)\n",
    "    \n",
    "    \n",
    "    elif dataset=='METRO':\n",
    "        \n",
    "        metro = pd.read_csv(path.join('datasets/data', 'MetroPT3.csv'))\n",
    "        metro.timestamp = pd.to_datetime(metro.timestamp)\n",
    "\n",
    "        start_attack = ['2020-04-18 00:00:00', '2020-05-29 23:30:00', '2020-06-05 10:00:00', '2020-07-15 14:30:00']\n",
    "        end_attack = ['2020-04-18 23:59:00', '2020-05-30 06:00:00', '2020-06-07 14:30:00', '2020-07-15 19:00:00']\n",
    "        label = np.zeros(metro.shape[0])\n",
    "        for i in range(4):\n",
    "            label += ((metro.timestamp>=start_attack[i] ) & (metro.timestamp<=end_attack[i])).values\n",
    "        label = label==1\n",
    "\n",
    "        test_mask = (metro.timestamp >= '2020-04-17 00:00:00') & (metro.timestamp <= '2020-07-16 00:00:00')\n",
    "        train_mask = np.logical_not(test_mask)\n",
    "        metro_test = metro[test_mask]\n",
    "        test_label = label[test_mask]\n",
    "        metro_train = metro[train_mask]\n",
    "        train_label = label[train_mask]\n",
    "\n",
    "        if args.cut < 1:\n",
    "            print('Cutting the dataset at ' + str(args.cut) + ' length \\n')\n",
    "            metro_train = metro_train.iloc[:int(len(metro_train)*args.cut)]\n",
    "            metro_test = metro_test.iloc[:int(len(metro_test)*args.cut)]\n",
    "            train_label = train_label[:int(len(train_label)*args.cut)]\n",
    "            test_label = test_label[:int(len(test_label)*args.cut)]\n",
    "        sample_rate = args.resample_rate\n",
    "        if sample_rate<=0 or sample_rate>1:\n",
    "            print('Incorrect resample rate, defaulting to 1\\n')\n",
    "            sample_rate = 1\n",
    "        else:\n",
    "            print('resampling to one observation every '+ str(int(1/sample_rate)))\n",
    "\n",
    "        metro_train = metro_train.iloc[::int(1/sample_rate)]#resampling\n",
    "        metro_test = metro_test.iloc[::int(1/sample_rate)]#resampling    \n",
    "        train_label = train_label[::int(1/sample_rate)]#resampling\n",
    "        test_label = test_label[::int(1/sample_rate)]#resampling\n",
    "\n",
    "        train_values = metro_train.iloc[:,2:].values\n",
    "        test_values = metro_test.iloc[:,2:].values\n",
    "\n",
    "        if args.scaler == 'quantile':\n",
    "            from sklearn.preprocessing  import QuantileTransformer\n",
    "            scaler = QuantileTransformer(output_distribution='uniform')\n",
    "        if args.scaler =='standard':\n",
    "            from sklearn.preprocessing  import StandardScaler\n",
    "            scaler = StandardScaler()\n",
    "        else:\n",
    "            from sklearn.preprocessing  import MinMaxScaler\n",
    "            scaler = MinMaxScaler()\n",
    "        \n",
    "        train_values = scaler.fit_transform(train_values)\n",
    "        test_values = scaler.transform(test_values) \n",
    "\n",
    "\n",
    "        #dump train values into file\n",
    "        makedirs('datasets/data/processed', exist_ok=True)\n",
    "        path_pkl = path.join('datasets/data/processed', 'METRO_train.pkl')\n",
    "        with open(path_pkl, 'wb') as file:\n",
    "            dump(train_values, file)\n",
    "\n",
    "\n",
    "        #dump test values into file\n",
    "        path_pkl = path.join('datasets/data/processed', 'METRO_test.pkl')\n",
    "        with open(path_pkl, 'wb') as file:\n",
    "            dump(test_values, file)\n",
    "\n",
    "\n",
    "        #dump test labels into file\n",
    "        path_pkl = path.join('datasets/data/processed', 'METRO_test_label.pkl')\n",
    "        with open(path_pkl, 'wb') as file:\n",
    "           dump(test_label, file)\n",
    "\n",
    "    elif dataset == 'IVECO':\n",
    "        print('Processing IVECO dataset')\n",
    "        import os\n",
    "        dfs = []\n",
    "        for filename in os.listdir('datasets/data/IVECO'):\n",
    "            if filename.endswith('.csv'):\n",
    "                file_path = path.join('datasets/data/IVECO', filename)\n",
    "\n",
    "            df = pd.read_csv(file_path)\n",
    "            df['startOfSampling'] = pd.to_datetime(df['startOfSampling'])\n",
    "            df['endOfSampling'] = pd.to_datetime(df['endOfSampling'])\n",
    "            df = df.sort_values(by='startOfSampling')\n",
    "            dfs.append(df)\n",
    "\n",
    "        iveco_all = pd.concat(dfs)\n",
    "        iveco_all.reset_index(drop = True, inplace = True)\n",
    "        \n",
    "        anomaly_free_veichles = ['ZCFCR35A005525500', 'ZCFCR35A005544595','ZCFCR35A305520050','ZCFCR35A305549323','ZCFCR35A505551221']\n",
    "        claimed_veichles = [#'ZCFCS72A705490017', #MANCANTE\n",
    "            'ZCFCR35A605433257','ZCFCS35AX05524066','ZCFCR35A905456015','ZCFCR35A805535577']\n",
    "        claim_dates = [#'21/04/2023',\n",
    "            '14/07/2023','14/07/2023','22/09/2023','09/10/2023']\n",
    "        datetime_series = pd.to_datetime(claim_dates, format='%d/%m/%Y', utc=True )\n",
    "\n",
    "        claims = pd.DataFrame({'chassis': claimed_veichles,'claim_date': datetime_series})\n",
    "        \n",
    "\n",
    "\n",
    "        if args.cut < 1:\n",
    "            print('Cutting the dataset at ' + str(args.cut) + ' length \\n')\n",
    "            iveco_all = iveco_all.iloc[:int(len(iveco_all)*args.cut)]\n",
    "        sample_rate = args.resample_rate\n",
    "        if sample_rate<=0 or sample_rate>1:\n",
    "            print('Incorrect resample rate, defaulting to 1\\n')\n",
    "            sample_rate = 1\n",
    "        else:\n",
    "            print('resampling to one observation every '+ str(int(1/sample_rate)))\n",
    "\n",
    "        iveco_all = iveco_all.iloc[::int(1/sample_rate)]#resampling\n",
    "        #labels = (swat['Normal/Attack'].values=='Attack')\n",
    "\n",
    "        #na handling\n",
    "        thresh = 0.75\n",
    "        na = []\n",
    "        for i in range(len(iveco_all.columns)):\n",
    "            na.append(sum(iveco_all.iloc[:,i].isna()))\n",
    "        where = (np.array(na) < iveco_all.shape[0]*thresh)\n",
    "\n",
    "        iveco_all = iveco_all.iloc[:, where]\n",
    "        \n",
    "        iveco_working = iveco_all[iveco_all['chassis'].isin(anomaly_free_veichles)]\n",
    "        iveco_claimed = iveco_all[iveco_all['chassis'].isin(claimed_veichles)]\n",
    "        \n",
    "        move_claims = True\n",
    "        if move_claims:\n",
    "            anomaly_dfs = []\n",
    "            working_dfs = []\n",
    "            for vehicle in claimed_veichles:#moving date from claimed vehicles to not anomaly after the claim date\n",
    "                vehicle_df = iveco_claimed[iveco_claimed['chassis']==vehicle] #extracting vehicle from df\n",
    "                claim_date = claims[claims['chassis']==vehicle]['claim_date'].reset_index(drop=True)[0] #extracting the claim date\n",
    "                working_df = vehicle_df[vehicle_df['endOfSampling']>claim_date] #extracting the portion after the claim\n",
    "                not_working_df = vehicle_df[vehicle_df['endOfSampling']<claim_date] #extracting the portion bevore the claim\n",
    "\n",
    "                working_dfs.append(working_df)\n",
    "                anomaly_dfs.append(not_working_df)\n",
    "\n",
    "            \n",
    "            working_df_from_claim = pd.concat(working_dfs).reset_index(drop = True) #the resulting dataset of extracted data\n",
    "            print(f\"Appending after claim vehicles ({working_df_from_claim.shape}) shape\")\n",
    "            iveco_working = pd.concat([iveco_working, working_df_from_claim]).reset_index(drop=True) #appending to existing anomaly free data\n",
    "            iveco_claimed = pd.concat(anomaly_dfs).reset_index(drop = True) #the anomaly dataset (claimed vehicles before claim)\n",
    "\n",
    "        iveco_working = iveco_working.iloc[:,5:] #esclusione colonne non informative\n",
    "        iveco_claimed = iveco_claimed.iloc[:,5:] #esclusione colonne non informative\n",
    "\n",
    "\n",
    "        if args.scaler == 'quantile':\n",
    "            from sklearn.preprocessing  import QuantileTransformer\n",
    "            scaler = QuantileTransformer(output_distribution='normal')\n",
    "        else:\n",
    "            from sklearn.preprocessing  import MinMaxScaler\n",
    "            scaler = MinMaxScaler()\n",
    "        \n",
    "        train_values = scaler.fit_transform(iveco_working.values)\n",
    "        test_values = scaler.transform(iveco_claimed.values)\n",
    "        #spectral residual data cleaning\n",
    "        if args.spectral_residual:\n",
    "            for i in range(values.shape[1]):\n",
    "                values[:,i] = spectral_residual_replace(values[:,i])\n",
    "\n",
    "        #train_labels = labels[:int(train_test_split*len(labels))]\n",
    "\n",
    "        # if args.no_anomaly_train:\n",
    "        #     print('removing anomalies from training data')\n",
    "        #     train_values = train_values[train_labels==False]\n",
    "        #test_labels = labels[int(train_test_split*len(labels)):]\n",
    "\n",
    "        #dump train values into file\n",
    "        makedirs('datasets/data/processed', exist_ok=True)\n",
    "        path_pkl = path.join('datasets/data/processed', 'IVECO_train.pkl')\n",
    "        with open(path_pkl, 'wb') as file:\n",
    "            print('Dumping training data into file')\n",
    "            print(f\"Train value shape: ({train_values.shape})\")\n",
    "            dump(train_values, file)\n",
    "\n",
    "\n",
    "        #dump test values into file\n",
    "        path_pkl = path.join('datasets/data/processed', 'IVECO_test.pkl')\n",
    "        with open(path_pkl, 'wb') as file:\n",
    "            print('Dumping test data into file')\n",
    "            print(f\"test value shape: ({test_values.shape})\")\n",
    "            dump(test_values, file)\n",
    "\n",
    "        #dump test labels into file (NB ALL FALSE FOR UNSUPERVISED DATA)\n",
    "        path_pkl = path.join('datasets/data/processed', 'IVECO_test_label.pkl')\n",
    "        test_labels = np.full(len(test_values), False, dtype=bool) #ALL FALSE VALUES FOR UNSUPERVISED DATA\n",
    "        test_labels[-1]=True #for calculating-auc-roc\n",
    "        with open(path_pkl, 'wb') as file:\n",
    "            print('Dumping test label into file')\n",
    "            dump(test_labels, file)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f1d7d7d-3a7e-49f9-a1f6-7655b0ddb72b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Spectral Residual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "c27f012d-3fde-42d0-a53f-4bdc37952392",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Spectral residual implementation for simple univariate outlier detection https://arxiv.org/pdf/1906.03821.pdf\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "def spectral_residual_replace(x, tau=2, window_size=20):\n",
    "    #compute fourier transform\n",
    "    fft_result = np.fft.fft(x)\n",
    "\n",
    "    #compute phase and log amplitude of fft\n",
    "    log_amplitude = np.log(np.abs(fft_result)) \n",
    "    phase = np.angle(fft_result)\n",
    "    \n",
    "    #smooth the amplitude and compute the residual\n",
    "    smoothed_log_amplitude = np.convolve(log_amplitude, np.ones(window_size)/window_size, mode = 'same')\n",
    "    residual_log_amplitude = smoothed_log_amplitude-log_amplitude\n",
    "    \n",
    "    #compute the spectral residual\n",
    "    im_unit = 1j\n",
    "    sr = np.abs(np.fft.ifft(np.exp(residual_log_amplitude + im_unit*phase)))\n",
    "    \n",
    "    #standardize the spectral residual\n",
    "    scaler = StandardScaler()\n",
    "    sr = scaler.fit_transform(sr.reshape(-1,1)).reshape(-1)\n",
    "    \n",
    "    #identify outliers (sr is now a 0-1 normal distribution)\n",
    "    outliers =  (sr > tau)\n",
    "    \n",
    "    #replace outliers\n",
    "    x_replaced = x.copy()\n",
    "    x_replaced[outliers] = np.mean(x) \n",
    "\n",
    "    return x_replaced\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2674bae9-3d1c-4152-b68f-f7bd90db408b",
   "metadata": {},
   "source": [
    "## Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "3167d6f7-fea5-4db6-b2d3-53d8a301607c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing IVECO dataset\n",
      "resampling to one observation every 1\n",
      "Appending after claim vehicles ((89, 200)) shape\n",
      "Dumping training data into file\n",
      "Train value shape: ((2262, 195))\n",
      "Dumping test data into file\n",
      "test value shape: ((6743, 195))\n",
      "Dumping test label into file\n"
     ]
    }
   ],
   "source": [
    "args = args_class()\n",
    "dataset = 'IVECO'\n",
    "load_data(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07fa344c-77e8-4dc0-9755-9d1a9d335a48",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "mtad-gat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
